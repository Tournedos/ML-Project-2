{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "import load_data\n",
    "import nan_imputation\n",
    "import helpers\n",
    "from helpers import find_repo_root\n",
    "from sksurv.nonparametric import kaplan_meier_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(load_data)\n",
    "importlib.reload(nan_imputation)\n",
    "importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 : Load the Data, we load all Lifespan folder expect of Terebafin\n",
    "> We don't load terebafin, we can see and modify this in the code of Load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = find_repo_root()\n",
    "repo_root\n",
    "\n",
    "data_path = os.path.join(repo_root, 'Data/Lifespan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "worms = load_data.load_lifespan(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a check print on worm 3 (companyDrug)\n",
    "worm_name = 'worm_3'  # Change this to the name of the worm you want to print\n",
    "print(f\"Worm: {worm_name}\")\n",
    "worm_data = worms[worm_name]\n",
    "df = pd.DataFrame(worm_data.T, columns=['Frame', 'Speed', 'X', 'Y', 'Changed Pixels', 'Category'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.print_fdict_summary(worms)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : NaN imputation\n",
    "> impute only on X and Y columns since only where there are NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, lifespan_array in worms.items(): \n",
    "    print(f\"Processing {name}\")\n",
    "    lifespan_arrayxy = lifespan_array[2:4,:]  # Extract columns for X and Y\n",
    "    missing_sequences = nan_imputation.count_successive_missing(lifespan_arrayxy)\n",
    "    for start, end, length in missing_sequences:\n",
    "        print(f\"  Missing sequence starts at column {start}, ends at column {end - 1}, length: {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lifespan_arrayxy)\n",
    "print(f\"Missing sequences for {name}: {missing_sequences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows to check for missing values (2:4 in zero-based indexing)\n",
    "rows_to_check = slice(2, 4)  # Rows 2 and 3 not row 4\n",
    "\n",
    "# Apply cut_array to each worm in the dataset\n",
    "cut_nan_dict = {name: nan_imputation.cut_array(array, rows_to_check) for name, array in worms.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the filtered arrays\n",
    "for name, item in cut_nan_dict.items():\n",
    "    print(f'{name} : {item.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a check print --> If we check we do have the number of frames decreased (because NaNs where removed) --> example with worm_3\n",
    "worm_name = 'worm_3'  # Change this to the name of the worm you want to print\n",
    "print(f\"Worm: {worm_name}\")\n",
    "worm_data = cut_nan_dict[worm_name]\n",
    "df = pd.DataFrame(worm_data.T, columns=['Frame', 'Speed', 'X', 'Y', 'Changed Pixels', 'Category'])\n",
    "\n",
    "# Check for NaN values in the DataFrame\n",
    "if df.isna().sum().sum() == 0:\n",
    "    print(f\"Worm {worm_name} has no NaN values after NaN imputation.\")\n",
    "else:\n",
    "    print(f\"Worm {worm_name} still contains NaN values.\")\n",
    "df\n",
    "\n",
    "#And we see that the total number of frames is decreased "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 : Figure out when do the worms die\n",
    ">When we find out on which frame he dies, drop the frames after his death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import isdead\n",
    "importlib.reload(isdead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement_threshold = 1.0 # Threshold for inactivity detection\n",
    "processed_worms = {} # Dictionary to store processed worms\n",
    "\n",
    "dying_times = []\n",
    "dying_times_frames = []\n",
    "\n",
    "# Use the cleaned data from nan_imputation\n",
    "cleaned_worms = cut_nan_dict  # Replace with the variable holding your cleaned data\n",
    "\n",
    "# Iterate through each worm in the dataset\n",
    "for worm_name, worm_data in cleaned_worms.items():\n",
    "    print(f\"Processing {worm_name}...\")\n",
    "    # Transpose worm_data for DataFrame creation\n",
    "    df_worm = pd.DataFrame(worm_data.T,columns=['Frame', 'Speed', 'X', 'Y', 'Changed Pixels', 'Category']) # Transpose the array\n",
    "\n",
    "    result = isdead.estimate_dying_time(df_worm, movement_threshold) # Use the estimate_dying_time function to find the dying frame\n",
    "    if result[0] is None:\n",
    "        print(f\"  {worm_name}: No inactivity detected. Retaining all data.\")\n",
    "        processed_worms[worm_name] = worm_data\n",
    "        continue\n",
    "\n",
    "    dying_frame, absolute_frame, dying_time_hours, segment_number = result\n",
    "  \n",
    "    dying_times.append(dying_time_hours) # Append dying time in hours to the list\n",
    "    dying_times_frames.append(absolute_frame) # Append the absolute frame to the list\n",
    "\n",
    "    print(f\"  {worm_name}: Dying frame = {dying_frame} of Segment = {segment_number}, Absolute frame = {absolute_frame}, Dying time = {dying_time_hours:.2f} hours\") # Print details\n",
    "\n",
    "    # Truncate the data up to the dying frame\n",
    "    truncated_data = worm_data[:, worm_data[0, :] <= dying_frame]\n",
    "    processed_worms[worm_name] = truncated_data\n",
    "\n",
    "# Print summary of processed worms\n",
    "print(\"\\nSummary of processed worms:\")\n",
    "for name, data in processed_worms.items():\n",
    "    print(f\"{name}: Original frames = {worms[name].shape[1]}, After truncation = {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dying_times_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a check print --> Check worm 3\n",
    "worm_name = 'worm_3'  # Change this to the name of the worm you want to print\n",
    "print(f\"Worm: {worm_name}\")\n",
    "worm_data = processed_worms[worm_name]\n",
    "df = pd.DataFrame(worm_data.T, columns=['Frame', 'Speed', 'X', 'Y', 'Changed Pixels', 'Category'])\n",
    "df\n",
    "\n",
    "# this for a movement threshold of 1.0\n",
    "# check worm 3 : Loading Data = 64794 --> Removing NaNs = 64533 frames --> Removing dead franes = 62175 Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the survival curve\n",
    "dying_times_sorted = sorted(dying_times) # Sort the dying times in ascending order\n",
    "\n",
    "# Compute the survival rate\n",
    "survival_rate = [1 - (i / len(dying_times_sorted)) for i in range(len(dying_times_sorted))]\n",
    "\n",
    "# Plot the survival curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(dying_times_sorted, survival_rate, marker='o', linestyle='-', color='blue')\n",
    "plt.xlabel('Dying Time (Hours)')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.title('Survival Curve')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builing X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(dying_times_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg = np.array(dying_times_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_features import preprocess_dataset\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Input\n",
    "from keras.layers import AveragePooling1D\n",
    "from keras.models import Model\n",
    "#from tensorflow.keras.utils import plot_model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step : EARLY LIFESPAN. Loading and Preprocessing the Dataset. Appends this truncated and reduced array to samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for name, item in processed_worms.items():\n",
    "    #print(name)\n",
    "    #print(item.T[0,1:5])\n",
    "    arr = item.T[:30000,1:5]\n",
    "    samples.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step : Features extraction. Calls the function preprocess_dataset(samples) to process the data and extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset\n",
    "features = list(preprocess_dataset(samples))\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "features_df = pd.DataFrame(features[0:54])\n",
    "\n",
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(features))\n",
    "print (y_reg) # shape of (36, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(features[0:len(features)]) # convert to DataFrame shape will be (36, nbr of features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step : Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_reg, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 25 worms in train and 11 in test. And we have 25 features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP : Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdsc = StandardScaler()\n",
    "stdsc.fit(X_train)\n",
    "X_train_std = stdsc.transform(X_train)\n",
    "X_test_std = stdsc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step : Model 1 Linear Regression, calculate MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_lr = model.predict(X_test)\n",
    "print('MAE:', mean_absolute_error(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larr = np.array(y_pred_lr)\n",
    "stat = np.full(larr.shape, True)\n",
    "time, survival_prob, conf_int = kaplan_meier_estimator(\n",
    "    stat, larr, conf_type=\"log-log\"\n",
    ")\n",
    "\n",
    "larrtr = np.array(y_test)\n",
    "stattr = np.full(larrtr.shape, True)\n",
    "time2, survival_prob2, conf_int2 = kaplan_meier_estimator(\n",
    "    stattr, larrtr, conf_type=\"log-log\"\n",
    ")\n",
    "\n",
    "plt.step(time, survival_prob, where=\"post\", label='model prediction')\n",
    "plt.fill_between(time, conf_int[0], conf_int[1], alpha=0.25, step=\"post\")\n",
    "plt.step(time2, survival_prob2, where=\"post\", label='real times')\n",
    "plt.fill_between(time2, conf_int2[0], conf_int2[1], alpha=0.25, step=\"post\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(r\"est. probability of survival $\\hat{S}(t)$\")\n",
    "plt.xlabel(\"time $t$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step : Model 2 : Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=1.0)  # Use Lasso(alpha=1.0) for feature selection\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_rr = model.predict(X_test)\n",
    "print('MAE:', mean_absolute_error(y_test, y_pred_rr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larr = np.array(y_pred_rr)\n",
    "stat = np.full(larr.shape, True)\n",
    "time, survival_prob, conf_int = kaplan_meier_estimator(\n",
    "    stat, larr, conf_type=\"log-log\"\n",
    ")\n",
    "\n",
    "larrtr = np.array(y_test)\n",
    "stattr = np.full(larrtr.shape, True)\n",
    "time2, survival_prob2, conf_int2 = kaplan_meier_estimator(\n",
    "    stattr, larrtr, conf_type=\"log-log\"\n",
    ")\n",
    "\n",
    "plt.step(time, survival_prob, where=\"post\", label='model prediction')\n",
    "plt.fill_between(time, conf_int[0], conf_int[1], alpha=0.25, step=\"post\")\n",
    "plt.step(time2, survival_prob2, where=\"post\", label='real times')\n",
    "plt.fill_between(time2, conf_int2[0], conf_int2[1], alpha=0.25, step=\"post\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(r\"est. probability of survival $\\hat{S}(t)$\")\n",
    "plt.xlabel(\"time $t$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step : model = DecisionTree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = DecisionTreeRegressor(max_depth=5)\n",
    "model.fit(X_train_std, y_train)\n",
    "y_pred_dtr = model.predict(X_test_std)\n",
    "print('MAE:', mean_absolute_error(y_test, y_pred_dtr))\n",
    "len(y_pred_dtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larr = np.array(y_pred_dtr)\n",
    "stat = np.full(larr.shape, True)\n",
    "time, survival_prob, conf_int = kaplan_meier_estimator(\n",
    "    stat, larr, conf_type=\"log-log\"\n",
    ")\n",
    "\n",
    "larrtr = np.array(y_test)\n",
    "stattr = np.full(larrtr.shape, True)\n",
    "time2, survival_prob2, conf_int2 = kaplan_meier_estimator(\n",
    "    stattr, larrtr, conf_type=\"log-log\"\n",
    ")\n",
    "\n",
    "plt.step(time, survival_prob, where=\"post\", label='model prediction')\n",
    "plt.fill_between(time, conf_int[0], conf_int[1], alpha=0.25, step=\"post\")\n",
    "plt.step(time2, survival_prob2, where=\"post\", label='real times')\n",
    "plt.fill_between(time2, conf_int2[0], conf_int2[1], alpha=0.25, step=\"post\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(r\"est. probability of survival $\\hat{S}(t)$\")\n",
    "plt.xlabel(\"time $t$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step : Model ; random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_rfr = model.predict(X_test)\n",
    "print('MAE:', mean_absolute_error(y_test, y_pred_dtr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larr = np.array(y_pred_rfr)\n",
    "stat = np.full(larr.shape, True)\n",
    "time, survival_prob, conf_int = kaplan_meier_estimator(\n",
    "    stat, larr, conf_type=\"log-log\"\n",
    ")\n",
    "\n",
    "larrtr = np.array(y_test)\n",
    "stattr = np.full(larrtr.shape, True)\n",
    "time2, survival_prob2, conf_int2 = kaplan_meier_estimator(\n",
    "    stattr, larrtr, conf_type=\"log-log\"\n",
    ")\n",
    "\n",
    "plt.step(time, survival_prob, where=\"post\", label='model prediction')\n",
    "plt.fill_between(time, conf_int[0], conf_int[1], alpha=0.25, step=\"post\")\n",
    "plt.step(time2, survival_prob2, where=\"post\", label='real times')\n",
    "plt.fill_between(time2, conf_int2[0], conf_int2[1], alpha=0.25, step=\"post\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(r\"est. probability of survival $\\hat{S}(t)$\")\n",
    "plt.xlabel(\"time $t$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step : model = SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_svm = model.predict(X_test)\n",
    "print('MAE:', mean_absolute_error(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larr = np.array(y_pred_svm)\n",
    "stat = np.full(larr.shape, True)\n",
    "time, survival_prob, conf_int = kaplan_meier_estimator(\n",
    "    stat, larr, conf_type=\"log-log\"\n",
    ")\n",
    "\n",
    "larrtr = np.array(y_test)\n",
    "stattr = np.full(larrtr.shape, True)\n",
    "time2, survival_prob2, conf_int2 = kaplan_meier_estimator(\n",
    "    stattr, larrtr, conf_type=\"log-log\"\n",
    ")\n",
    "\n",
    "plt.step(time, survival_prob, where=\"post\", label='model prediction')\n",
    "plt.fill_between(time, conf_int[0], conf_int[1], alpha=0.25, step=\"post\")\n",
    "plt.step(time2, survival_prob2, where=\"post\", label='real times')\n",
    "plt.fill_between(time2, conf_int2[0], conf_int2[1], alpha=0.25, step=\"post\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(r\"est. probability of survival $\\hat{S}(t)$\")\n",
    "plt.xlabel(\"time $t$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X : Print all features Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
