{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from models import *   \n",
    "import load_data\n",
    "import nan_imputation\n",
    "import helpers\n",
    "from helpers import find_repo_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload isdead.py\n",
    "import importlib\n",
    "importlib.reload(load_data)\n",
    "importlib.reload(nan_imputation)\n",
    "importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 : Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = find_repo_root()\n",
    "repo_root\n",
    "\n",
    "data_path = os.path.join(repo_root, 'Data/Lifespan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "worms = load_data.load_lifespan(data_path)\n",
    "#worms.pop(\"worm_1_companyDrug\", None)  # The second argument avoids KeyError if the key doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a check print on worm 3 (companyDrug)\n",
    "worm_name = 'worm_3'  # Change this to the name of the worm you want to print\n",
    "print(f\"Worm: {worm_name}\")\n",
    "worm_data = worms[worm_name]\n",
    "df = pd.DataFrame(worm_data.T, columns=['Frame', 'Speed', 'X', 'Y', 'Changed Pixels', 'Category'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.print_fdict_summary(worms)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : NaN imputation\n",
    "> impute only on X and Y columns since only where there are NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, lifespan_array in worms.items(): \n",
    "    print(f\"Processing {name}\")\n",
    "    lifespan_arrayxy = lifespan_array[2:4,:]  # Extract columns for X and Y\n",
    "    missing_sequences = nan_imputation.count_successive_missing(lifespan_arrayxy)\n",
    "    for start, end, length in missing_sequences:\n",
    "        print(f\"  Missing sequence starts at column {start}, ends at column {end - 1}, length: {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lifespan_arrayxy)\n",
    "print(f\"Missing sequences for {name}: {missing_sequences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows to check for missing values (2:4 in zero-based indexing)\n",
    "rows_to_check = slice(2, 4)  # Rows 2 and 3 not row 4\n",
    "\n",
    "# Apply cut_array to each worm in the dataset\n",
    "cut_nan_dict = {name: nan_imputation.cut_array(array, rows_to_check) for name, array in worms.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the filtered arrays\n",
    "for name, item in cut_nan_dict.items():\n",
    "    print(f'{name} : {item.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a check print --> If we check we do have the number of frames decreased (because NaNs where removed) --> example with worm_3\n",
    "worm_name = 'worm_3'  # Change this to the name of the worm you want to print\n",
    "print(f\"Worm: {worm_name}\")\n",
    "worm_data = cut_nan_dict[worm_name]\n",
    "df = pd.DataFrame(worm_data.T, columns=['Frame', 'Speed', 'X', 'Y', 'Changed Pixels', 'Category'])\n",
    "\n",
    "# Check for NaN values in the DataFrame\n",
    "if df.isna().sum().sum() == 0:\n",
    "    print(f\"Worm {worm_name} has no NaN values after NaN imputation.\")\n",
    "else:\n",
    "    print(f\"Worm {worm_name} still contains NaN values.\")\n",
    "\n",
    "df\n",
    "\n",
    "#And we see that the total number of frames is decreased "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 : Figure out when do the worms die\n",
    ">When we find out on which frame he dies, drop the frames after his death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import isdead\n",
    "importlib.reload(isdead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement_threshold = 1.0 # Threshold for inactivity detection\n",
    "processed_worms = {} # Dictionary to store processed worms\n",
    "\n",
    "dying_times = []\n",
    "\n",
    "# Use the cleaned data from nan_imputation\n",
    "cleaned_worms = cut_nan_dict  # Replace with the variable holding your cleaned data\n",
    "\n",
    "# Iterate through each worm in the dataset\n",
    "for worm_name, worm_data in cleaned_worms.items():\n",
    "    print(f\"Processing {worm_name}...\")\n",
    "    # Transpose worm_data for DataFrame creation\n",
    "    df_worm = pd.DataFrame(worm_data.T,columns=['Frame', 'Speed', 'X', 'Y', 'Changed Pixels', 'Category']) # Transpose the array\n",
    "\n",
    "    result = isdead.estimate_dying_time(df_worm, movement_threshold) # Use the estimate_dying_time function to find the dying frame\n",
    "    if result[0] is None:\n",
    "        print(f\"  {worm_name}: No inactivity detected. Retaining all data.\")\n",
    "        processed_worms[worm_name] = worm_data\n",
    "        continue\n",
    "\n",
    "    dying_frame, absolute_frame, dying_time_hours, segment_number = result\n",
    "  \n",
    "    dying_times.append(dying_time_hours) # Append dying time in hours to the list\n",
    "\n",
    "    print(f\"  {worm_name}: Dying frame = {dying_frame} of Segment = {segment_number}, Absolute frame = {absolute_frame}, Dying time = {dying_time_hours:.2f} hours\") # Print details\n",
    "\n",
    "    # Truncate the data up to the dying frame\n",
    "    truncated_data = worm_data[:, worm_data[0, :] <= dying_frame]\n",
    "    processed_worms[worm_name] = truncated_data\n",
    "\n",
    "# Print summary of processed worms\n",
    "print(\"\\nSummary of processed worms:\")\n",
    "for name, data in processed_worms.items():\n",
    "    print(f\"{name}: Original frames = {worms[name].shape[1]}, After truncation = {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a check print --> Check worm 3\n",
    "worm_name = 'worm_3'  # Change this to the name of the worm you want to print\n",
    "print(f\"Worm: {worm_name}\")\n",
    "worm_data = processed_worms[worm_name]\n",
    "df = pd.DataFrame(worm_data.T, columns=['Frame', 'Speed', 'X', 'Y', 'Changed Pixels', 'Category'])\n",
    "df\n",
    "\n",
    "# this for a movement threshold of 1.0\n",
    "# check worm 3 : Loading Data = 64794 --> Removing NaNs = 64533 frames --> Removing dead franes = 62175 Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the survival curve\n",
    "dying_times_sorted = sorted(dying_times) # Sort the dying times in ascending order\n",
    "\n",
    "# Compute the survival rate\n",
    "survival_rate = [1 - (i / len(dying_times_sorted)) for i in range(len(dying_times_sorted))]\n",
    "\n",
    "# Plot the survival curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(dying_times_sorted, survival_rate, marker='o', linestyle='-', color='blue')\n",
    "plt.xlabel('Dying Time (Hours)')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.title('Survival Curve')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "# Prepare data for the plot\n",
    "original_lengths = [worms[name].shape[1] for name in worms.keys()]\n",
    "truncated_lengths = [processed_worms[name].shape[1] for name in processed_worms.keys()]\n",
    "worm_ids = list(worms.keys())\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(worm_ids, original_lengths, label='Original Lifespan', alpha=0.7)\n",
    "plt.bar(worm_ids, truncated_lengths, label='Truncated Lifespan', alpha=0.7)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Worms')\n",
    "plt.ylabel('Number of Frames')\n",
    "plt.title('Original vs Truncated Lifespan for Each Worm')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the indices of the features to standardize (e.g., Speed, X, Y)\n",
    "feature_columns = [1, 2, 3, 4]  # Assuming 1 = Speed, 2 = X, 3 = Y, 4 = Changed Pixels, not standardizing the frame nummber = 0 and the catetgory = 5\n",
    "\n",
    "# Apply per-worm standardization\n",
    "standardized_worms = helpers.standardization(processed_worms, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the worm data\n",
    "worm_3_data = standardized_worms['worm_3']  # Assuming worms is your dictionary of worm data\n",
    "\n",
    "# Convert to a DataFrame for easier inspection\n",
    "columns = ['Frame', 'Speed', 'X', 'Y', 'Changed Pixels', 'Category']\n",
    "df_worm_3 = pd.DataFrame(worm_3_data.T, columns=columns)  # Transpose for proper orientation\n",
    "\n",
    "df_worm_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 : Splitting the Data\n",
    "> Split the worms in train worms and test worms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_worms, test_worms = load_data.split_worms(standardized_worms, test_size=0.2)\n",
    "\n",
    "print(f\"Training Worms: {list(train_worms.keys())}\")\n",
    "print(f\"Testing Worms: {list(test_worms.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 : Load only early Lifespan for train set and keep the whole lifespan for test (validation set)\n",
    "> Now we will load only a portion of the worms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_fraction = 0.4\n",
    "\n",
    "#early_train_worms = load_data.load_earlylifespan(train_worms, data_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the worm data\n",
    "worm_3_data = early_train_worms['worm_3']  # Assuming worms is your dictionary of worm data\n",
    "\n",
    "# Convert to a DataFrame for easier inspection\n",
    "columns = ['Frame', 'Speed', 'X', 'Y', 'Changed Pixels', 'Category']\n",
    "df_worm_3 = pd.DataFrame(worm_3_data.T, columns=columns)  # Transpose for proper orientation\n",
    "\n",
    "df_worm_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 : Prepare data for Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6.1 : Separate features X and target Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for worm_name, worm_data in train_worms.items():\n",
    "    # Convert to DataFrame for better readability\n",
    "    df = pd.DataFrame(\n",
    "        worm_data.T, \n",
    "        columns=['Frame', 'Speed', 'X', 'Y', 'Changed Pixels', 'Category']\n",
    "    )\n",
    "    \n",
    "    # Use the already extracted early lifespan data as features\n",
    "    X_train.append(df[['Speed', 'X', 'Y', 'Changed Pixels']].values)\n",
    "    \n",
    "    # Total lifespan (number of frames) is the target\n",
    "    y_train.append(len(df))\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X_train = np.array(X_train, dtype=object)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(f\"Training data prepared: {len(X_train)} worms with variable-length sequences.\")\n",
    "\n",
    "# Prepare test data (similar process)\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for worm_name, worm_data in test_worms.items():\n",
    "    df = pd.DataFrame(\n",
    "        worm_data.T, \n",
    "        columns=['Frame', 'Speed', 'X', 'Y', 'Changed Pixels', 'Category']\n",
    "    )\n",
    "    X_test.append(df[['Speed', 'X', 'Y', 'Changed Pixels']].values)\n",
    "    y_test.append(len(df))\n",
    "\n",
    "X_test = np.array(X_test, dtype=object)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f\"Test data prepared: {len(X_test)} worms with variable-length sequences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6.2 : Truncate early lifespan on X_train,X_test but keep Y_train,Y_test full length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fraction = 0.4\n",
    "early_X_train = load_data.truncate_lifespan(X_train, data_fraction)\n",
    "early_X_test = load_data.truncate_lifespan(X_test, data_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimension check\n",
    "# Check dimensions of X_train and X_test\n",
    "print(f\"Number of worms in X_train: {len(early_X_train)}\")\n",
    "print(f\"Number of worms in X_test: {len(early_X_test)}\")\n",
    "\n",
    "# Check lengths of sequences for a few worms\n",
    "print(\"Lengths of sequences in X_train (first 5 worms):\")\n",
    "print([len(worm) for worm in X_train[:5]])\n",
    "\n",
    "print(\"Lengths of sequences in X_test (first 5 worms):\")\n",
    "print([len(worm) for worm in X_test[:5]])\n",
    "\n",
    "# Check dimensions of y_train and y_test\n",
    "print(f\"Number of worms in y_train: {len(y_train)}\")\n",
    "print(f\"Number of worms in y_test: {len(y_test)}\")\n",
    "\n",
    "# Verify alignment between features and targets\n",
    "assert len(X_train) == len(y_train), \"Mismatch: X_train and y_train do not align!\"\n",
    "assert len(X_test) == len(y_test), \"Mismatch: X_test and y_test do not align!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6.2 : Flattend data for simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten X_train and X_test for models requiring fixed-length input\n",
    "X_train_flat = np.concatenate(X_train)\n",
    "y_train_flat = np.repeat(y_train, [len(x) for x in X_train])\n",
    "\n",
    "X_test_flat = np.concatenate(X_test)\n",
    "y_test_flat = np.repeat(y_test, [len(x) for x in X_test])\n",
    "\n",
    "print(f\"Flattened X_train shape: {X_train_flat.shape}\")\n",
    "print(f\"Flattened y_train shape: {y_train_flat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6.3 : Train and evaluate the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
